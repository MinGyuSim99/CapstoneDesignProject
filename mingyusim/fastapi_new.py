import sys, os
current_dir = os.path.dirname(__file__)
workspace_root = os.path.abspath(os.path.join(current_dir, os.pardir))
if workspace_root not in sys.path:
    sys.path.append(workspace_root)

import multiprocessing as mp
if __name__ != "__main__":
    try:
        mp.set_start_method("spawn", force=True)
        print("âœ… (Main) Multiprocessing start method set to 'spawn'.")
    except RuntimeError:
        pass

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks, Request
from fastapi.responses import JSONResponse, FileResponse
from openai import OpenAI
import base64, re, random, requests, time, asyncio, shutil, uuid, hashlib
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from pathlib import Path
import torch
from multiprocessing import Process, Queue, Manager
from SadTalker111.inference_module import (init_models, run_inference_fast)

# --- ê²½ë¡œ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---
ffmpeg_path = os.path.expanduser("~/ffmpeg-7.0.2-amd64-static")
if os.path.exists(ffmpeg_path):
    os.environ["PATH"] = f"{ffmpeg_path}:{os.environ['PATH']}"
else:
    print("âš ï¸ ê²½ê³ : ì§€ì •ëœ ffmpeg ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

dotenv_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path)
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
ELEVEN_KEY = os.getenv("ELEVEN_API_KEY")

BASE_DIR = Path(__file__).parent.resolve()
RESULTS_DIR = BASE_DIR / "results"
GREET_DIR = BASE_DIR / "greet_audios" 
RESULTS_DIR.mkdir(exist_ok=True)
GREET_DIR.mkdir(exist_ok=True) 

client = OpenAI(api_key=OPENAI_KEY)

# âœ¨âœ¨âœ¨ [ì—…ë°ì´íŠ¸] ìš”ì²­í•˜ì‹  5ì¢… ë³´ì´ìŠ¤ë¥¼ í¬í•¨í•˜ì—¬ ì „ì²´ ë³´ì´ìŠ¤ ëª©ë¡ì„ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤. âœ¨âœ¨âœ¨
MY_VOICE_IDS = {
    # --- ê¸°ë³¸ ë³´ì´ìŠ¤ ---
    "bong_pal": {"id": "PLfpgtLkFW07fDYbUiRJ", "tags": ["ë‚¨", "ë…¸ë…„", "ì°¨ë¶„"]},
    "min_ho":   {"id": "U1cJYS4EdbaHmfR7YzHd", "tags": ["ë‚¨", "ì¤‘ë…„", "ë‹¨ì •"]},
    "hyuk":     {"id": "ZJCNdZEjYwkOElxugmW2", "tags": ["ë‚¨", "ì¤‘ë…„", "í¸ì•ˆ"]},
    
    # --- 1ì°¨ ì¶”ê°€ ë³´ì´ìŠ¤ ---
    "anna_teacher": {"id": "Cx2PEJFdr8frSuUVB6yZ", "tags": ["ì—¬", "ì²­ë…„", "í™œë°œ", "ê¸ì •ì "]},
    "richard_narrator": {"id": "6pVydnYcVtMsrrSeUKs6", "tags": ["ë‚¨", "ì¤‘ë…„", "ë‚´ë ˆì´ì…˜"]},
    "sarah_character": {"id": "emSmWzY0c0xtx5IFMCVv", "tags": ["ì—¬", "ì²­ë…„", "í™œë°œ", "ìºë¦­í„°"]},
    "brittney_peppy": {"id": "XiPS9cXxAVbaIWtGDHDh", "tags": ["ì—¬", "ì²­ë…„", "í™œë°œ", "ì •ë³´ì„±"]},
    "shelby_british": {"id": "rfkTsdZrVWEVhDycUYn9", "tags": ["ì—¬", "ì²­ë…„", "ì°¨ë¶„", "ì •ë³´ì„±", "ë‚´ë ˆì´ì…˜"]},
    "brittney_social": {"id": "kPzsL2i3teMYv0FxEYQ6", "tags": ["ì—¬", "ì²­ë…„", "í™œë°œ", "ì •ë³´ì„±"]},
    "grandfather_namchun_kind": {"id": "5ON5Fnz24cnOozEQfGAm", "tags": ["ë‚¨", "ë…¸ë…„", "ì°¨ë¶„", "ì¹œì ˆí•œ", "ë‚´ë ˆì´ì…˜"]},
    "grandfather_namchun_dark": {"id": "FQ3MuLxZh0jHcZmA5vW1", "tags": ["ë‚¨", "ë…¸ë…„", "ì§„ì§€í•œ", "ì†ì‚­ì´ëŠ”"]},
    "deoksu_character": {"id": "IAETYMYM3nJvjnlkVTKI", "tags": ["ë‚¨", "ì¤‘ë…„", "ìºë¦­í„°", "ê·€ì—¬ìš´"]},
    "jangho_husky": {"id": "UmYoqGlufKxhJ6NCx5Mv", "tags": ["ë‚¨", "ì¤‘ë…„", "ê±°ì¹œ", "í—ˆìŠ¤í‚¤"]},
    "annakim_narrator": {"id": "uyVNoMrnUku1dZyVEXwD", "tags": ["ì—¬", "ì²­ë…„", "ë‚´ë ˆì´ì…˜"]},
    "kkc_narrator": {"id": "1W00IGEmNmwmsDeYy7ag", "tags": ["ë‚¨", "ì²­ë…„", "í™œë°œ", "ì•ˆì •ì ì¸", "ë‚´ë ˆì´ì…˜"]},
    "kyungduk_authoritative": {"id": "2gbExjiWDnG1DMGr81Bx", "tags": ["ë‚¨", "ì¤‘ë…„", "ê¶Œìœ„ì ì¸", "ì§„ì§€í•œ", "ì •ë³´ì„±"]},
    "hunmin_soft": {"id": "MpbDJfQJUYUnp0i1QvOZ", "tags": ["ë‚¨", "ì²­ë…„", "ì°¨ë¶„", "ë¶€ë“œëŸ¬ìš´", "ë‚´ë ˆì´ì…˜"]},
    "deokpal_rough": {"id": "wzMVIc8FAFmgMxFpN0uM", "tags": ["ë‚¨", "ì¤‘ë…„", "ê±°ì¹œ", "ë‚¨ì„±ì ì¸", "ë‚´ë ˆì´ì…˜"]},
    "jennie_informative": {"id": "z6Kj0hecH20CdetSElRT", "tags": ["ì—¬", "ì²­ë…„", "ì •ë³´ì„±", "í™œë°œ"]},
    "seulki_calm": {"id": "ksaI0TCD9BstzEzlxj4q", "tags": ["ì—¬", "ì²­ë…„", "ì°¨ë¶„", "ë‚´ë ˆì´ì…˜"]},
    "seer_morganna": {"id": "7NsaqHdLuKNFvEfjpUno", "tags": ["ì—¬", "ë…¸ë…„", "ì‹ ë¹„ë¡œìš´", "ìºë¦­í„°"]},
    
    # --- 2ì°¨ ì¶”ê°€ ë³´ì´ìŠ¤ ---
    "lola_storyteller": {"id": "gILcvhAz18uV9ARSsU4u", "tags": ["ì—¬", "ì¤‘ë…„", "í™œë°œ", "ìºë¦­í„°", "ë‚´ë ˆì´ì…˜"]},
    "nanay_avelina": {"id": "HXiggO6rHDAxWaFMzhB7", "tags": ["ì—¬", "ì¤‘ë…„", "ì°¨ë¶„", "ë”°ëœ»í•œ", "ì¹œì ˆí•œ"]},
    "jacqui_aussie": {"id": "lNABL6eI3BpPT8BvSqjK", "tags": ["ì—¬", "ì²­ë…„", "í¸ì•ˆ", "ë‚´ë ˆì´ì…˜"]},
    "tosca_deep": {"id": "fNmw8sukfGuvWVOp33Ge", "tags": ["ì—¬", "ë…¸ë…„", "ì°¨ë¶„", "ì§„ì§€í•œ", "ë‚´ë ˆì´ì…˜"]},
    "angelina_storyteller": {"id": "MLpDWJvrjFIdb63xbJp8", "tags": ["ì—¬", "ì²­ë…„", "ì°¨ë¶„", "ë¶€ë“œëŸ¬ìš´", "ë”°ëœ»í•œ", "ë‚´ë ˆì´ì…˜"]},
}

try:
    VOICE_CACHE = requests.get("https://api.elevenlabs.io/v1/voices", headers={"xi-api-key": ELEVEN_KEY, "Accept": "application/json"}, timeout=15).json()["voices"]
    print(f"âœ… ElevenLabsì—ì„œ {len(VOICE_CACHE)}ê°œì˜ ëª©ì†Œë¦¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"Error fetching ElevenLabs voices: {e}")
    VOICE_CACHE = []

# --- í—¬í¼ í•¨ìˆ˜ ---
def parse_profile(text: str):
    t = text.lower()
    gender = "ì—¬" if re.search(r"ì—¬ì„±|ì—¬ì", t) else "ë‚¨" if re.search(r"ë‚¨ì„±|ë‚¨ì", t) else None
    if re.search(r"ì•„ì´|ì–´ë¦°|ì†Œë…„|ì†Œë…€", t): age = "ì•„ë™"
    elif re.search(r"10ëŒ€|20ëŒ€|ì²­ë…„|ì Š", t): age = "ì²­ë…„"
    elif re.search(r"30ëŒ€|40ëŒ€|ì¤‘ë…„", t): age = "ì¤‘ë…„"
    elif re.search(r"50ëŒ€|60ëŒ€|70ëŒ€|ë…¸ì¸|ë…¸ë…„", t): age = "ë…¸ë…„"
    else: age = None
    pers_list = ["ì°¨ë¶„", "ì¹¨ì°©", "í™œë°œ", "ë°", "ì¾Œí™œ", "ì§€ì ", "í•™êµ¬", "ê¸ì •ì ", "ì¹œì ˆí•œ", "ì§„ì§€í•œ", "ì‹ ë¹„ë¡œìš´", "ê¶Œìœ„ì ì¸", "ë¶€ë“œëŸ¬ìš´", "í¸ì•ˆ", "ì†ì‚­ì´ëŠ”", "ê±°ì¹œ", "í—ˆìŠ¤í‚¤", "ë‚¨ì„±ì ì¸", "ê·€ì—¬ìš´", "ë‚´ë ˆì´ì…˜", "ìºë¦­í„°", "ì •ë³´ì„±", "ë‹¨ì •", "ë”°ëœ»í•œ"]
    found_pers = [p for p in pers_list if p in t]
    personality = found_pers[0] if found_pers else None
    return gender, age, personality

def choose_voice_unified(profile_txt: str):
    g, a, p = parse_profile(profile_txt)
    print(f"\nğŸ•µï¸  í”„ë¡œí•„ ë¶„ì„ ì‹œì‘: ì„±ë³„='{g}', ë‚˜ì´='{a}', ì„±ê²©='{p}'")
    all_voices = []
    for name, details in MY_VOICE_IDS.items(): all_voices.append({"voice_id": details["id"], "labels_txt": " ".join(details["tags"]), "is_my_voice": True, "name": f"(ë‚´ ëª©ì†Œë¦¬) {name}"})
    for voice in VOICE_CACHE:
        labels_txt = (voice.get("name", "") + " " + " ".join(map(str, voice.get("labels", {}).values()))).lower()
        all_voices.append({"voice_id": voice["voice_id"], "labels_txt": labels_txt, "is_my_voice": False, "name": voice.get("name", "Unknown")})
    
    gender_map = {"ì—¬": ["female", "woman", "girl", "ì—¬"], "ë‚¨": ["male", "man", "boy", "ë‚¨"]}
    age_map = {"ì•„ë™": ["child", "kid", "ì•„ë™"], "ì²­ë…„": ["young", "teen", "ì²­ë…„"], "ì¤‘ë…„": ["middle", "middle-aged", "ì¤‘ë…„"], "ë…¸ë…„": ["old", "elder", "ë…¸ë…„"]}
    pers_map = {
        "ì°¨ë¶„": ["calm", "soft", "ì°¨ë¶„", "ì¹¨ì°©"], "í™œë°œ": ["bright", "cheerful", "í™œë°œ", "ë°", "ì¾Œí™œ"], "ì§€ì ": ["smart", "intelligent", "ì§€ì ", "í•™êµ¬"],
        "ê¸ì •ì ": ["positive", "ê¸ì •ì "], "ì¹œì ˆí•œ": ["kind", "friendly", "ì¹œì ˆí•œ"], "ì§„ì§€í•œ": ["serious", "ì§„ì§€í•œ"], "ì‹ ë¹„ë¡œìš´": ["mysterious", "ì‹ ë¹„ë¡œìš´"],
        "ê¶Œìœ„ì ì¸": ["authoritative", "ê¶Œìœ„ì ì¸"], "ë¶€ë“œëŸ¬ìš´": ["soft", "ë¶€ë“œëŸ¬ìš´"], "í¸ì•ˆ": ["comfortable", "í¸ì•ˆ"], "ì†ì‚­ì´ëŠ”": ["whispering", "ì†ì‚­ì´ëŠ”"],
        "ê±°ì¹œ": ["rough", "ê±°ì¹œ"], "í—ˆìŠ¤í‚¤": ["husky", "í—ˆìŠ¤í‚¤"], "ë‚¨ì„±ì ì¸": ["masculine", "ë‚¨ì„±ì ì¸"], "ê·€ì—¬ìš´": ["cute", "ê·€ì—¬ìš´"],
        "ë‚´ë ˆì´ì…˜": ["narration", "ë‚´ë ˆì´ì…˜"], "ìºë¦­í„°": ["character", "ìºë¦­í„°"], "ì •ë³´ì„±": ["informative", "ì •ë³´ì„±"], "ë‹¨ì •": ["neat", "ë‹¨ì •"], "ë”°ëœ»í•œ": ["warm", "ë”°ëœ»í•œ"]
    }
    best_voices, best_score = [], -1.0
    for voice in all_voices:
        labels, score = voice["labels_txt"], 0.0
        if g and any(k in labels for k in gender_map.get(g, [])): score += 1
        if a and any(k in labels for k in age_map.get(a, [])): score += 1
        if p and p in pers_map and any(k in labels for k in pers_map[p]): score += 1
        if voice["is_my_voice"] and score > 0: score += 0.5
        if score > best_score: best_score, best_voices = score, [voice]
        elif score == best_score and score > 0: best_voices.append(voice)
    
    if best_voices:
        chosen_voice = random.choice(best_voices)
        print(f"ğŸ† ìµœì¢… ì„ íƒëœ ëª©ì†Œë¦¬: '{chosen_voice['name']}' (ID: {chosen_voice['voice_id']}), ì ìˆ˜: {best_score:.1f}\n")
        return chosen_voice['voice_id']
    else:
        print(f"âš ï¸ ë§¤ì¹­ë˜ëŠ” ëª©ì†Œë¦¬ë¥¼ ì°¾ì§€ ëª»í•´ ê¸°ë³¸ ëª©ì†Œë¦¬('bong_pal')ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")
        return MY_VOICE_IDS['bong_pal']['id']

def get_profile(img): 
    prompt = """
ë„ˆëŠ” ê·¸ë¦¼ ì† ì¸ë¬¼ì— ì–´ìš¸ë¦¬ëŠ” ëª©ì†Œë¦¬ë¥¼ ì°¾ì•„ì£¼ëŠ” 'ë³´ì´ìŠ¤ ìºìŠ¤íŒ… AI'ë‹¤.
ì•„ë˜ ë¶„ë¥˜ ê¸°ì¤€ì— ë”°ë¼ ê·¸ë¦¼ ì† ì¸ë¬¼ì˜ í”„ë¡œí•„ì„ ë°˜ë“œì‹œ ì¶”ì¸¡í•´ì„œ, ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ì¡°í•©ìœ¼ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ë¼.
---ë¶„ë¥˜ ê¸°ì¤€---
1.  **ì„±ë³„:** ë‚¨, ì—¬
2.  **ë‚˜ì´ëŒ€:** ì•„ë™, ì²­ë…„, ì¤‘ë…„, ë…¸ë…„
3.  **ì„±ê²© (ì•„ë˜ ëª©ë¡ì—ì„œ ê°€ì¥ ì–´ìš¸ë¦¬ëŠ” ê²ƒì„ 1~2ê°œ ì„ íƒ):**
    - **ë¶„ìœ„ê¸°:** ì°¨ë¶„, í™œë°œ, ê¸ì •ì , ì¹œì ˆí•œ, ì§„ì§€í•œ, ì‹ ë¹„ë¡œìš´, ê¶Œìœ„ì ì¸, ë¶€ë“œëŸ¬ìš´, í¸ì•ˆ, ë”°ëœ»í•œ
    - **ëª©ì†Œë¦¬ íŠ¹ì§•:** ì†ì‚­ì´ëŠ”, ê±°ì¹œ, í—ˆìŠ¤í‚¤, ë‚¨ì„±ì ì¸, ê·€ì—¬ìš´
    - **ìš©ë„:** ë‚´ë ˆì´ì…˜, ìºë¦­í„°, ì •ë³´ì„±, ë‹¨ì •
---ì¶œë ¥ í˜•ì‹---
- ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ë¼.
- ì˜ˆì‹œ: 'ì´ ì¸ë¬¼ì€ ë‚´ë ˆì´ì…˜ì— ì–´ìš¸ë¦¬ëŠ”, ì°¨ë¶„í•˜ê³  ê¶Œìœ„ì ì¸ ëª©ì†Œë¦¬ì˜ ë…¸ë…„ ë‚¨ì„±ì…ë‹ˆë‹¤.'
- 'ëª¨ë¥´ê² ë‹¤' ë˜ëŠ” 'ì¶”ì¸¡í•˜ê¸° ì–´ë µë‹¤'ì™€ ê°™ì€ ë‹µë³€ì€ ì ˆëŒ€ í•˜ë©´ ì•ˆ ëœë‹¤. ë¬´ì¡°ê±´ ì¶”ì¸¡í•´ì„œ ë‹µí•´ë¼.
"""
    return client.chat.completions.create(model="gpt-4o", messages=[{"role": "user", "content": [{"type": "text", "text": prompt}, img]}]).choices[0].message.content.strip()
def get_dialogue(img): return client.chat.completions.create(model="gpt-4o", messages=[{"role": "user", "content": [{"type": "text", "text": "ë„ˆëŠ” ì´ ë¯¸ìˆ  ì‘í’ˆ ê·¸ë¦¼ ì†ì˜ ì¸ë¬¼ì´ì•¼. ì„¤ëª… ì—†ì´ ë°”ë¡œ ì–´ë¦°ì´ì—ê²Œ ë§ì„ ê±´ë„¤ëŠ” 1ì¸ì¹­ ëŒ€ì‚¬ë¡œ ì‘í’ˆì„¤ëª…ê³¼ ì‘ê°€ì—ëŒ€í•œ ì„¤ëª…ì„ ê°„ë‹¨íˆ 2ì¤„ë¡œ ë§í•´.\n- GPTì²˜ëŸ¼ 'ì£„ì†¡í•˜ì§€ë§Œ' í˜¹ì€ ì„¤ëª…Â·ì„œë¡  ì ˆëŒ€ ì“°ì§€ ë§ˆ.\n- 'ì•ˆë…• ë‚˜ëŠ” ~ ì´/ê°€ ê·¸ë¦° ~ì•¼' ê°™ì€ ë§íˆ¬ë¡œ ë¬´ì¡°ê±´ ì•„ì´ì—ê²Œ ë§í•˜ë“¯ì´ ë”°ëœ»í•˜ê³  ì¹œì ˆí•˜ê²Œ."}, img]}]).choices[0].message.content.strip()
def get_artist_name(img): return client.chat.completions.create(model="gpt-4o", messages=[{"role": "user", "content": [{"type": "text", "text": "ì´ ê·¸ë¦¼ì˜ ì‘ê°€ ì´ë¦„ë§Œ ì •í™•í•˜ê²Œ ë§í•´ì¤˜. ë”± ì‘ê°€ ì´ë¦„ë§Œ ì •í™•íˆ í•œ ì¤„ë¡œ. ì˜ˆì‹œ: ë¹ˆì„¼íŠ¸ ë°˜ ê³ í"}, img]}]).choices[0].message.content.strip()
def has_batchim(korean_name: str) -> bool:
    if not korean_name: return False
    ch = korean_name[-1]
    if 'ê°€' <= ch <= 'í£': return (ord(ch) - 0xAC00) % 28 != 0
    return False
def vocative(name: str) -> str: return f"{name}{'ì•„' if has_batchim(name) else 'ì•¼'}"

# âœ¨âœ¨âœ¨ [ìˆ˜ì •] ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤. âœ¨âœ¨âœ¨
def worker_process_loop(job_queue: Queue, greet_video_cache: dict, final_video_cache: dict, sadtalker_paths: dict, device: str):
    print("âœ… (Worker) ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    from SadTalker111.src.test_audio2coeff import Audio2Coeff
    from SadTalker111.src.facerender.animate import AnimateFromCoeff
    audio_to_coeff, animate_from_coeff = Audio2Coeff(sadtalker_paths, device), AnimateFromCoeff(sadtalker_paths, device)
    print(f"âœ… (Worker) ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Device: {device}). ì‘ì—… ëŒ€ê¸° ì¤‘...")
    while True:
        try:
            job_details = job_queue.get()
            if job_details is None: break
            job_id, job_type = job_details['job_id'], job_details['type']
            print(f"[{job_id}] (Worker) â–¶â–¶â–¶ '{job_type}' ì˜ìƒ ìƒì„± ì‘ì—… ì‹œì‘")
            
            # âœ¨ [ìˆ˜ì •] run_inference_fastì— í•„ìš”í•œ ì¸ìë§Œ ì •í™•íˆ ê³¨ë¼ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.
            inference_args = {
                'audio_to_coeff': audio_to_coeff, 
                'animate_from_coeff': animate_from_coeff,
                'source_image_path': job_details['source_image_path'], 
                'crop_pic_path': job_details['crop_pic_path'],
                'coeff_path': job_details['coeff_path'], 
                'audio_path': job_details['audio_path'],
                'crop_info': job_details['crop_info'],
            }
            temp_video_path = run_inference_fast(**inference_args)
            
            dest_video_path = job_details['job_dir'] / f"{job_type}_video.mp4"
            shutil.move(temp_video_path, dest_video_path)
            print(f"[{job_id}] (Worker) ê²°ê³¼ ì˜ìƒ ì €ì¥: {dest_video_path}")
            
            if job_type == "final": final_video_cache[job_id] = str(dest_video_path)
            else: greet_video_cache[job_id] = str(dest_video_path)
        except Exception as e:
            print(f"(Worker) ERROR: {e}")
            if 'job_id' in locals():
                job_type = locals().get('job_type', 'unknown')
                if job_type == "final": final_video_cache[job_id] = {"error": str(e)}
                else: greet_video_cache[job_id] = {"error": str(e)}
    print("âš ï¸ (Worker) ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ.")

@asynccontextmanager
async def lifespan(app: FastAPI):
    manager = Manager()
    app.state.GREET_VIDEO_CACHE, app.state.FINAL_VIDEO_CACHE, app.state.DESC_TTS_CACHE = manager.dict(), manager.dict(), manager.dict()
    app.state.job_queue = Queue()
    print("âœ… (Main) í”„ë¡œì„¸ìŠ¤ ê³µìœ  ìºì‹œ ë° í ìƒì„± ì™„ë£Œ.")
    print("â³ (Main) SadTalker ëª¨ë¸ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¡œë”©í•©ë‹ˆë‹¤...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    checkpoint_dir = "/home/team06/workspace/SadTalker111/checkpoints"
    if not os.path.exists(checkpoint_dir): raise FileNotFoundError(f"SadTalker ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬({checkpoint_dir})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sadtalker_init_data = await asyncio.to_thread(init_models, checkpoint_dir, device, 256, False, "full")
    app.state.preprocess_model = sadtalker_init_data["preprocess_model"]
    app.state.sadtalker_paths = sadtalker_init_data["sadtalker_paths"]
    print("âœ… (Main) Preprocess ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    worker = Process(target=worker_process_loop, args=(app.state.job_queue, app.state.GREET_VIDEO_CACHE, app.state.FINAL_VIDEO_CACHE, app.state.sadtalker_paths, device), daemon=True)
    worker.start()
    app.state.worker = worker
    yield
    print("âš ï¸ (Main) ì„œë²„ ì¢…ë£Œ ì ˆì°¨ ì‹œì‘...")
    app.state.job_queue.put(None)
    app.state.worker.join(timeout=5)
    print("âœ… (Main) ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì •ìƒ ì¢…ë£Œ.")

async def background_content_pipeline(job_id: str, nickname: str, job_dir: Path, source_image_path: Path, crop_pic_path: Path, coeff_path: str, crop_info: list, app_state):
    try:
        print(f"[{job_id}] (Main) â–¶â–¶â–¶ 'ì„¤ëª…' ì½˜í…ì¸  ì¤€ë¹„ ì‹œì‘")
        with open(crop_pic_path, "rb") as f: b64img = base64.b64encode(f.read()).decode("ascii")
        img_payload = {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64img}"}}
        with ThreadPoolExecutor() as executor:
            f_profile = executor.submit(get_profile, img_payload)
            f_dialogue = executor.submit(get_dialogue, img_payload)
            f_artist = executor.submit(get_artist_name, img_payload)
            profile_text, dialogue_text, artist_name = f_profile.result(), f_dialogue.result(), f_artist.result()
            print(f"[{job_id}] (Main) [GPT] í”„ë¡œí•„ í…ìŠ¤íŠ¸: '{profile_text}'")
            voice_id = choose_voice_unified(profile_text)
            print(f"[{job_id}] (Main) ğŸš€ 'ì„¤ëª…' TTS ìš”ì²­ ì‹œì‘ (Voice ID: {voice_id})")
            headers = {"xi-api-key": ELEVEN_KEY, "Content-Type": "application/json"}
            tts_settings = {"stability": 0.3, "similarity_boost": 0.85}
            body = {"text": dialogue_text, "model_id": "eleven_multilingual_v2", "voice_settings": tts_settings}
            desc_audio_content = requests.post(f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}", headers=headers, json=body).content
            print(f"[{job_id}] (Main) âœ… 'ì„¤ëª…' TTS ì²˜ë¦¬ ì™„ë£Œ")
        desc_audio_path = job_dir / "description_audio.mp3"
        desc_audio_path.write_bytes(desc_audio_content)
        app_state.DESC_TTS_CACHE[job_id] = {"result": dialogue_text, "profile": profile_text, "artist": artist_name}
        common_job_data = {'coeff_path': coeff_path, 'crop_pic_path': str(crop_pic_path), 'crop_info': crop_info, 'source_image_path': str(source_image_path), 'job_dir': job_dir}
        app_state.job_queue.put({**common_job_data, 'job_id': job_id, 'type': 'final', 'audio_path': str(desc_audio_path)})
        print(f"[{job_id}] (Main) âœ… 'ìµœì¢…' ì˜ìƒ ì‘ì—…ì„ íì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"(Main) [background_content_pipeline] ERROR: {e}")
        app_state.FINAL_VIDEO_CACHE[job_id] = {"error": str(e)}

app = FastAPI(lifespan=lifespan)

@app.post("/create-greet")
def create_greet(nickname: str = Form(...)):
    nick_clean = nickname.strip()
    hash8 = hashlib.sha1(nick_clean.encode()).hexdigest()[:8]
    mp3_path = GREET_DIR / f"greet_{hash8}.mp3"
    if not mp3_path.exists():
        text = f"{vocative(nick_clean)} ì•„ì•ˆë…•? í€´ì¦ˆë¥¼ ë§ì¶”ë©´ ë‚´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ë§ˆ"
        headers = {"xi-api-key": ELEVEN_KEY, "accept": "audio/mpeg", "Content-Type": "application/json"}
        # âœ¨âœ¨âœ¨ [ì—…ë°ì´íŠ¸] ì¸ì‚¬ë§ ëª©ì†Œë¦¬ë¥¼ 'Tosca'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. âœ¨âœ¨âœ¨
        voice_id = 'fNmw8sukfGuvWVOp33Ge' # Tosca
        body = {"model_id": "eleven_multilingual_v2", "text": text, "voice_settings": {}}
        try:
            r = requests.post(f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream", headers=headers, json=body, timeout=20)
            r.raise_for_status()
            mp3_path.write_bytes(r.content)
            print(f"âœ… (Main) ì¸ì‚¬ë§ ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ: {mp3_path}")
        except Exception as e:
            return JSONResponse({"ok": False, "error": str(e)}, status_code=500)
    return {"ok": True}

@app.post("/preprocess")
async def preprocess(request: Request, background_tasks: BackgroundTasks, image: UploadFile = File(...), nickname: str = Form(...)):
    job_id = uuid.uuid4().hex
    job_dir = RESULTS_DIR / job_id
    job_dir.mkdir(exist_ok=True)
    source_image_path = job_dir / "source.jpg"
    await image.seek(0)
    source_image_path.write_bytes(await image.read())
    print(f"\n[{job_id}] (Main) ìƒˆ ì‘ì—… ì‹œì‘. Job Directory: {job_dir}")
    while not hasattr(request.app.state, 'preprocess_model'):
        await asyncio.sleep(0.1)
    coeff_path, crop_pic_path, crop_info = request.app.state.preprocess_model.generate(str(source_image_path), str(job_dir), "full", source_image_flag=True, pic_size=256)
    nick_clean = nickname.strip()
    hash8 = hashlib.sha1(nick_clean.encode()).hexdigest()[:8]
    greet_audio_path = GREET_DIR / f"greet_{hash8}.mp3"
    common_job_data = {'coeff_path': coeff_path, 'crop_pic_path': str(crop_pic_path), 'crop_info': crop_info, 'source_image_path': str(source_image_path), 'job_dir': job_dir}
    request.app.state.job_queue.put({**common_job_data, 'job_id': job_id, 'type': 'greet', 'audio_path': str(greet_audio_path)})
    print(f"[{job_id}] (Main) âœ… 'ì¸ì‚¬' ì˜ìƒ ì‘ì—…ì„ íì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    background_tasks.add_task(background_content_pipeline, job_id, nickname, job_dir, source_image_path, Path(crop_pic_path), coeff_path, crop_info, request.app.state)
    print(f"[{job_id}] (Main) âœ… 'ì„¤ëª…' ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸ì„ ë°±ê·¸ë¼ìš´ë“œì— ë“±ë¡.")
    return {"job_id": job_id}

# (GET ì—”ë“œí¬ì¸íŠ¸ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•©ë‹ˆë‹¤.)
@app.get("/get-greet-video")
async def get_greet_video(request: Request, job_id: str):
    for _ in range(200): 
        if job_id in request.app.state.GREET_VIDEO_CACHE:
            result = request.app.state.GREET_VIDEO_CACHE.get(job_id)
            if isinstance(result, str) and os.path.exists(result): return FileResponse(result, media_type="video/mp4")
            elif isinstance(result, dict) and 'error' in result: raise HTTPException(500, f"Video generation failed: {result['error']}")
        await asyncio.sleep(0.1)
    raise HTTPException(408, "Greet video not ready or timed out.")

@app.get("/get-description")
async def get_description(request: Request, job_id: str):
    for _ in range(150):
        if job_id in request.app.state.DESC_TTS_CACHE:
            result = request.app.state.DESC_TTS_CACHE.get(job_id)
            if result and 'error' not in result: return JSONResponse(result)
            elif result and 'error' in result: raise HTTPException(500, f"Description generation failed: {result['error']}")
        await asyncio.sleep(0.1)
    raise HTTPException(408, "Description not ready or timed out.")

@app.get("/get-final-video")
async def get_final_video(request: Request, job_id: str):
    for _ in range(450): 
        if job_id in request.app.state.FINAL_VIDEO_CACHE:
            result = request.app.state.FINAL_VIDEO_CACHE.get(job_id)
            if isinstance(result, str) and os.path.exists(result): return FileResponse(result, media_type="video/mp4")
            elif isinstance(result, dict) and 'error' in result: raise HTTPException(500, f"Video generation failed: {result['error']}")
        await asyncio.sleep(0.1)
    raise HTTPException(408, "Final video not ready or timed out.")


# --- ì„œë²„ ì‹¤í–‰ (í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰) ---
# uvicorn fastapi_new:app --host 0.0.0.0 --port 5058